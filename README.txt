# TimeMachine - Time-Travel Debugger for LangGraph Agents

TimeMachine is a comprehensive debugging and recording system for LangGraph agents that captures every node execution with full state tracking. Think of it as a "flight recorder" for your AI agents.

## ğŸš€ Quick Start

### Installation
```bash
# Clone the repository
git clone <repository-url>
cd time-machine

# Activate virtual environment
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# Install dependencies (already included in venv)
pip install -r requirements.txt
```

### Basic Usage

#### Method 1: Decorator (Recommended)
```python
import timemachine
from langgraph.graph import StateGraph

@timemachine.record("my_recordings.db")
def create_my_agent():
    graph = StateGraph(MyState)
    graph.add_node("step1", my_function)
    graph.add_node("step2", another_function)
    # ... add edges ...
    return graph  # Return StateGraph, not compiled

# Use your agent normally
agent = create_my_agent()
result = agent.invoke({"messages": [], "data": "test"})
```

#### Method 2: Direct Wrapping
```python
import timemachine

# Wrap your existing graph
graph = StateGraph(MyState)
# ... build your graph ...

tm_graph = timemachine.TimeMachineGraph(graph, "recordings.db")
agent = tm_graph.compile()
result = agent.invoke(initial_state)
```

#### Method 3: Context Manager
```python
import timemachine

with timemachine.recording("context_recordings.db"):
    agent = create_my_agent()
    result = agent.invoke(initial_state)
```

## ğŸ“ Project Structure

### Current Structure (Phase 1)
```
time-machine/
â”œâ”€â”€ timemachine/              # Core TimeMachine library
â”‚   â”œâ”€â”€ __init__.py          # Public API
â”‚   â”œâ”€â”€ recorder.py          # SQLite recording engine
â”‚   â”œâ”€â”€ wrapper.py           # Node and graph wrappers
â”‚   â”œâ”€â”€ serializer.py        # State serialization
â”‚   â””â”€â”€ decorator.py         # Integration decorators
â”œâ”€â”€ test/                    # Test suite
â”‚   â”œâ”€â”€ README.md           # Test documentation
â”‚   â”œâ”€â”€ run_all_tests.py    # Test runner
â”‚   â”œâ”€â”€ test_basic_functionality.py
â”‚   â”œâ”€â”€ test_serialization.py
â”‚   â”œâ”€â”€ test_decorator_integration.py
â”‚   â”œâ”€â”€ test_context_manager.py
â”‚   â””â”€â”€ test_demo_sample_agent.py
â”œâ”€â”€ sample_agent.py         # Example LangGraph agent
â”œâ”€â”€ architecture.md         # Technical architecture
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .gitignore             # Git ignore patterns
â””â”€â”€ README.txt             # This file
```

### Future Structure (Full Product)
```
time-machine/
â”œâ”€â”€ timemachine/
â”‚   â”œâ”€â”€ __init__.py           # Main API exports
â”‚   â”œâ”€â”€ core/                 # Core recording functionality
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ recorder.py       # Database operations
â”‚   â”‚   â”œâ”€â”€ wrapper.py        # Node instrumentation
â”‚   â”‚   â”œâ”€â”€ serializer.py     # State serialization
â”‚   â”‚   â””â”€â”€ decorator.py      # Integration helpers
â”‚   â”œâ”€â”€ analysis/             # Phase 2: Data analysis
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_tracker.py    # LLM call detection
â”‚   â”‚   â”œâ”€â”€ cost_analyzer.py  # Token/cost tracking
â”‚   â”‚   â””â”€â”€ patterns.py       # Pattern detection
â”‚   â”œâ”€â”€ replay/               # Phase 2: Counterfactual engine
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ engine.py         # Replay orchestration
â”‚   â”‚   â”œâ”€â”€ counterfactual.py # "What if" scenarios
â”‚   â”‚   â””â”€â”€ cache.py          # Response caching
â”‚   â”œâ”€â”€ web/                  # Phase 3: Web UI backend
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api.py            # FastAPI routes
â”‚   â”‚   â”œâ”€â”€ models.py         # Pydantic models
â”‚   â”‚   â””â”€â”€ auth.py           # Authentication
â”‚   â””â”€â”€ utils/                # Shared utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py         # Configuration management
â”‚       â”œâ”€â”€ exceptions.py     # Custom exceptions
â”‚       â””â”€â”€ validators.py     # Input validation
â”œâ”€â”€ web/                      # Phase 3: Web UI frontend
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ api/
â”‚   â””â”€â”€ public/
â”œâ”€â”€ cli/                      # Command-line interface
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # Entry point
â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”œâ”€â”€ record.py        # Recording commands
â”‚   â”‚   â”œâ”€â”€ replay.py        # Replay commands
â”‚   â”‚   â””â”€â”€ analyze.py       # Analysis commands
â”œâ”€â”€ examples/                 # Usage examples
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ advanced_agent.py
â”‚   â””â”€â”€ integrations/
â”‚       â”œâ”€â”€ langchain_example.py
â”‚       â””â”€â”€ llamaindex_example.py
â”œâ”€â”€ test/                     # Tests
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ build/
â”‚   â””â”€â”€ api/
â””â”€â”€ scripts/                  # Development scripts
    â”œâ”€â”€ setup.py
    â”œâ”€â”€ build.py
    â””â”€â”€ deploy.py
```

## ğŸ” What Gets Recorded

TimeMachine captures comprehensive execution data:

### Node Execution Records
- **Execution ID**: Unique identifier for each node execution
- **Graph Run ID**: Groups all nodes from a single graph execution
- **Node Name**: Which node was executed
- **Timestamp**: When execution started
- **Input State**: Complete state passed to the node (JSON serialized)
- **Output State**: Complete state returned by the node (JSON serialized)
- **Duration**: Execution time in milliseconds
- **Status**: success/error/interrupted
- **Error Messages**: Full error details if execution failed

### State Serialization
- **LangChain Messages**: Proper handling of HumanMessage, AIMessage, SystemMessage
- **Complex Objects**: Nested dictionaries and lists
- **Type Preservation**: Message types and metadata maintained
- **JSON Storage**: Human-readable database format

## ğŸ§ª Testing

### Run All Tests
```bash
python test/run_all_tests.py
```

### Run Individual Tests
```bash
python test/test_basic_functionality.py     # Core functionality
python test/test_serialization.py          # State serialization
python test/test_decorator_integration.py  # Decorator approach
python test/test_context_manager.py        # Context manager
python test/test_demo_sample_agent.py      # Complete demo
```

### Test Coverage
- âœ… Node execution recording
- âœ… State serialization/deserialization
- âœ… Error handling and recovery
- âœ… All integration methods
- âœ… LangChain message handling
- âœ… Multi-node graph execution
- âœ… Database persistence

## ğŸ“Š Viewing Recordings

### Programmatic Access
```python
import timemachine

# Access recordings
recorder = timemachine.TimeMachineRecorder("my_recordings.db")

# List all graph runs
runs = recorder.list_graph_runs()
print(f"Found {len(runs)} graph executions")

# Get detailed execution data
for run in runs:
    graph_run_id = run['graph_run_id']
    executions = recorder.get_graph_executions(graph_run_id)
    
    print(f"Graph run {graph_run_id[:8]}...")
    for execution in executions:
        print(f"  - {execution['node_name']}: {execution['status']} ({execution['duration_ms']}ms)")
```

### Database Schema
SQLite database with three main tables:
- **`node_executions`**: Main execution records
- **`llm_calls`**: LLM interaction tracking (ready for Phase 2)
- **`graph_snapshots`**: Graph topology storage

## ğŸ—ï¸ Architecture

TimeMachine follows a modular architecture:

### Core Components
1. **TimeMachineNodeWrapper**: Wraps individual node functions for recording
2. **TimeMachineGraph**: Wraps entire StateGraph for automatic instrumentation
3. **TimeMachineRecorder**: Manages SQLite database operations
4. **StateSerializer**: Handles complex state object serialization

### Integration Methods
1. **Decorator**: `@timemachine.record()` - Zero-code-change approach
2. **Direct Wrapping**: `TimeMachineGraph(graph)` - Explicit control
3. **Context Manager**: `with timemachine.recording():` - Scope-based recording

### Key Features
- **Zero-Code-Change Integration**: Use decorators for existing agents
- **Automatic State Handling**: Complex LangGraph states serialized properly
- **Error Recovery**: Failed executions properly recorded with error details
- **Performance Tracking**: Millisecond-precision timing data
- **Cross-Platform**: Works on Windows, Linux, and macOS

## ğŸ”§ Implementation Status

### âœ… Phase 1: Core Recording (COMPLETE)
- [x] Node execution recording
- [x] State serialization
- [x] SQLite storage
- [x] Multiple integration methods
- [x] Error handling
- [x] Comprehensive testing

### ğŸš§ Phase 2: LLM Integration (PLANNED)
- [ ] LLM call detection within nodes
- [ ] Model parameter extraction
- [ ] Token usage tracking
- [ ] Cost estimation
- [ ] Response caching

### **Phase 2.5: Simplification (Next)**

Cut out unnecessary features like caching responses and tracking AI, and finding problems and waste. Focus only on "what if" games.

### ğŸš§ Phase 3: Web UI (PLANNED)
- [ ] Web-based execution viewer
- [ ] State inspection interface
- [ ] Timeline visualization
- [ ] Debugging tools
- [ ] Export capabilities

## ğŸ“ Sample Agent

The repository includes `sample_agent.py` - a simple two-step LangGraph agent that:
1. Asks for a topic (or uses mock input in tests)
2. Generates an AI response about that topic

Perfect for testing TimeMachine integration:

```python
# Run with TimeMachine recording
python test/test_demo_sample_agent.py
```

## ğŸ› ï¸ Development

### Requirements
- Python 3.11+
- LangGraph 0.2.68+
- LangChain Core 0.3.74+
- SQLAlchemy 2.0.43+
- OpenAI API key (for sample agent)

### Environment Setup
```bash
# Copy environment template
cp .env.example .env

# Add your OpenAI API key
OPENAI_API_KEY=your_key_here
```

### Running Tests
All tests are designed to work without external API calls (using mocks).

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run the test suite: `python test/run_all_tests.py`
5. Submit a pull request

## ğŸ“„ License

[License information would go here]

## ğŸ™‹ Support

For questions, issues, or feature requests:
1. Check the `test/` directory for examples
2. Review `architecture.md` for technical details
3. Open an issue on the repository

---

**TimeMachine** - Making LangGraph agent debugging as easy as time travel! ğŸ•°ï¸
"""
Counterfactual Analysis Engine for TimeMachine - Phase 2.5 Simplified
"What if" scenario analysis for LLM agent executions
Focus only on essential counterfactual features
"""
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from .engine import ReplayEngine, ReplayConfiguration, ReplayResult


class CounterfactualType(Enum):
    """Types of counterfactual analysis"""
    MODEL_CHANGE = "model_change"           # What if we used a different model?
    TEMPERATURE_CHANGE = "temperature_change"  # What if we used different temperature?
    PROMPT_CHANGE = "prompt_change"         # What if we changed the prompt?
    INPUT_CHANGE = "input_change"           # What if we had different inputs?
    PARAMETER_SWEEP = "parameter_sweep"     # What if we tried multiple parameter values?


@dataclass
class CounterfactualScenario:
    """Defines a counterfactual scenario"""
    name: str
    description: str
    type: CounterfactualType
    modifications: Dict[str, Any]
    expected_outcome: Optional[str] = None


@dataclass
class CounterfactualResult:
    """Result of counterfactual analysis"""
    scenario: CounterfactualScenario
    replay_result: ReplayResult
    analysis: Dict[str, Any]
    confidence: float


@dataclass
class CounterfactualComparison:
    """Comparison of multiple counterfactual scenarios"""
    original_execution_id: str
    scenarios: List[CounterfactualResult]
    recommendations: List[str]


class CounterfactualEngine:
    """Engine for running counterfactual analysis on recorded executions"""
    
    def __init__(self, replay_engine: Optional[ReplayEngine] = None):
        self.replay_engine = replay_engine or ReplayEngine()
        
    def analyze_model_alternatives(self, execution_id: str, 
                                 models: Optional[List[str]] = None) -> CounterfactualComparison:
        """Analyze what would happen with different models"""
        models = models or ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo', 'claude-3-sonnet']
        
        scenarios = []
        for model in models:
            scenario = CounterfactualScenario(
                name=f"Use {model}",
                description=f"What if we used {model} instead?",
                type=CounterfactualType.MODEL_CHANGE,
                modifications={'model_name': model},
                expected_outcome=f"Response generated by {model}"
            )
            scenarios.append(scenario)
        
        return self._run_scenarios(execution_id, scenarios)
    
    def analyze_temperature_sensitivity(self, execution_id: str,
                                      temperatures: Optional[List[float]] = None) -> CounterfactualComparison:
        """Analyze sensitivity to temperature changes"""
        temperatures = temperatures or [0.0, 0.3, 0.7, 1.0, 1.5]
        
        scenarios = []
        for temp in temperatures:
            scenario = CounterfactualScenario(
                name=f"Temperature {temp}",
                description=f"What if we used temperature={temp}?",
                type=CounterfactualType.TEMPERATURE_CHANGE,
                modifications={'temperature': temp},
                expected_outcome="More deterministic" if temp < 0.5 else "More creative"
            )
            scenarios.append(scenario)
        
        return self._run_scenarios(execution_id, scenarios)
    
    def analyze_prompt_variations(self, execution_id: str,
                                prompt_variations: List[Dict[str, str]]) -> CounterfactualComparison:
        """Analyze different prompt variations"""
        scenarios = []
        for i, variation in enumerate(prompt_variations):
            scenario = CounterfactualScenario(
                name=f"Prompt Variation {i+1}",
                description=variation.get('description', f"Prompt variation {i+1}"),
                type=CounterfactualType.PROMPT_CHANGE,
                modifications={'replace_inputs': {'0': variation['content']}},
                expected_outcome=variation.get('expected_outcome')
            )
            scenarios.append(scenario)
        
        return self._run_scenarios(execution_id, scenarios)
    
    def analyze_parameter_sweep(self, execution_id: str,
                               parameter: str,
                               values: List[Any]) -> CounterfactualComparison:
        """Analyze a sweep of parameter values"""
        scenarios = []
        for value in values:
            scenario = CounterfactualScenario(
                name=f"{parameter}={value}",
                description=f"What if {parameter} was {value}?",
                type=CounterfactualType.PARAMETER_SWEEP,
                modifications={parameter: value}
            )
            scenarios.append(scenario)
        
        return self._run_scenarios(execution_id, scenarios)
    
    # Note: Cost and quality optimization methods removed in Phase 2.5
    # Focus only on core "what if" scenario analysis
    
    def _enhance_prompt_for_quality(self, original_prompt: str) -> str:
        """Enhance a prompt for better quality"""
        enhancements = [
            "Please provide a detailed and accurate response.",
            "Think step by step and explain your reasoning.",
            "Include specific examples where relevant.",
            "Ensure your response is comprehensive and well-structured."
        ]
        
        return f"{original_prompt}\n\n{' '.join(enhancements)}"
    
    def _run_scenarios(self, execution_id: str, 
                      scenarios: List[CounterfactualScenario]) -> CounterfactualComparison:
        """Run multiple counterfactual scenarios and compare results"""
        results = []
        
        for scenario in scenarios:
            config = ReplayConfiguration(
                modify_llm_params=self._extract_llm_params(scenario.modifications),
                replace_inputs=scenario.modifications.get('replace_inputs'),
                modify_state=self._extract_state_modifications(scenario.modifications)
            )
            
            replay_result = self.replay_engine.replay_execution(execution_id, config)
            
            # Analyze the result
            analysis = self._analyze_scenario_result(scenario, replay_result)
            confidence = self._calculate_confidence(scenario, replay_result)
            
            result = CounterfactualResult(
                scenario=scenario,
                replay_result=replay_result,
                analysis=analysis,
                confidence=confidence
            )
            results.append(result)
        
        # Generate recommendations
        recommendations = self._generate_recommendations(results)
        
        return CounterfactualComparison(
            original_execution_id=execution_id,
            scenarios=results,
            recommendations=recommendations
        )
    
    def _extract_llm_params(self, modifications: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Extract LLM parameters from modifications"""
        llm_params = {}
        llm_param_keys = ['model_name', 'temperature', 'max_tokens', 'top_p', 'frequency_penalty', 'presence_penalty']
        
        for key in llm_param_keys:
            if key in modifications:
                llm_params[key] = modifications[key]
        
        return llm_params if llm_params else None
    
    def _extract_state_modifications(self, modifications: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Extract state modifications from modifications"""
        state_mods = {}
        llm_param_keys = ['model_name', 'temperature', 'max_tokens', 'top_p', 'frequency_penalty', 'presence_penalty']
        
        for key, value in modifications.items():
            if key not in llm_param_keys and key != 'replace_inputs':
                state_mods[key] = value
        
        return state_mods if state_mods else None
    
    def _analyze_scenario_result(self, scenario: CounterfactualScenario, 
                               replay_result: ReplayResult) -> Dict[str, Any]:
        """Analyze the result of a counterfactual scenario"""
        analysis = {
            'success': replay_result.success,
            'cost_impact': replay_result.cost_difference,
            'performance_impact': replay_result.duration_ms
        }
        
        # Scenario-specific analysis
        if scenario.type == CounterfactualType.MODEL_CHANGE:
            analysis['model_effectiveness'] = self._evaluate_model_change(replay_result)
        elif scenario.type == CounterfactualType.TEMPERATURE_CHANGE:
            analysis['temperature_impact'] = self._evaluate_temperature_change(replay_result)
        elif scenario.type == CounterfactualType.PROMPT_CHANGE:
            analysis['prompt_effectiveness'] = self._evaluate_prompt_change(replay_result)
        
        return analysis
    
    def _evaluate_model_change(self, replay_result: ReplayResult) -> str:
        """Evaluate the effectiveness of a model change"""
        if replay_result.cost_difference > 0:
            return "Higher cost model - evaluate if quality improvement justifies cost"
        elif replay_result.cost_difference < 0:
            return "Lower cost model - monitor quality impact"
        else:
            return "Similar cost - compare quality metrics"
    
    def _evaluate_temperature_change(self, replay_result: ReplayResult) -> str:
        """Evaluate the impact of temperature change"""
        if replay_result.success:
            return "Temperature change applied successfully"
        else:
            return "Temperature change resulted in failure"
    
    def _evaluate_prompt_change(self, replay_result: ReplayResult) -> str:
        """Evaluate the effectiveness of prompt changes"""
        if replay_result.success:
            return "Prompt change applied successfully"
        else:
            return "Prompt change resulted in failure"

    
    def _calculate_confidence(self, scenario: CounterfactualScenario,
                            replay_result: ReplayResult) -> float:
        """Calculate confidence score for a counterfactual result"""
        if not replay_result.success:
            return 0.1
        
        confidence = 0.8  # Base confidence for successful replays
        
        # Simple confidence calculation without difference score dependency
        if scenario.type == CounterfactualType.MODEL_CHANGE:
            confidence = 0.9
        elif scenario.type == CounterfactualType.TEMPERATURE_CHANGE:
            confidence = 0.85
        
        return max(0.0, min(1.0, confidence))
    


    
    def _generate_recommendations(self, results: List[CounterfactualResult]) -> List[str]:
        """Generate actionable recommendations - simplified for Phase 2.5"""
        recommendations = []
        
        failed_scenarios = [r for r in results if not r.replay_result.success]
        if failed_scenarios:
            recommendations.append("Some scenarios failed - review error handling and input validation")
        
        # Simple recommendation based on success rate
        successful_results = [r for r in results if r.replay_result.success]
        if len(successful_results) > len(results) * 0.8:
            recommendations.append("High success rate across scenarios - configurations appear stable")
        
        return recommendations

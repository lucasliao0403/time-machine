"""
Counterfactual Analysis Engine for TimeMachine - Phase 2.5 Simplified
"What if" scenario analysis for LLM agent executions
Focus only on essential counterfactual features
"""
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from .engine import ReplayEngine, ReplayConfiguration, ReplayResult


class CounterfactualType(Enum):
    """Types of counterfactual analysis"""
    MODEL_CHANGE = "model_change"           # What if we used a different model?
    TEMPERATURE_CHANGE = "temperature_change"  # What if we used different temperature?
    PROMPT_CHANGE = "prompt_change"         # What if we changed the prompt?
    INPUT_CHANGE = "input_change"           # What if we had different inputs?
    PARAMETER_SWEEP = "parameter_sweep"     # What if we tried multiple parameter values?


@dataclass
class CounterfactualScenario:
    """Defines a counterfactual scenario"""
    name: str
    description: str
    type: CounterfactualType
    modifications: Dict[str, Any]
    expected_outcome: Optional[str] = None


@dataclass
class CounterfactualResult:
    """Result of counterfactual analysis"""
    scenario: CounterfactualScenario
    replay_result: ReplayResult
    analysis: Dict[str, Any]
    insights: List[str]
    confidence: float


@dataclass
class CounterfactualComparison:
    """Comparison of multiple counterfactual scenarios"""
    original_execution_id: str
    scenarios: List[CounterfactualResult]
    best_scenario: Optional[CounterfactualResult]
    worst_scenario: Optional[CounterfactualResult]
    insights: List[str]
    recommendations: List[str]


class CounterfactualEngine:
    """Engine for running counterfactual analysis on recorded executions"""
    
    def __init__(self, replay_engine: Optional[ReplayEngine] = None):
        self.replay_engine = replay_engine or ReplayEngine()
        
    def analyze_model_alternatives(self, execution_id: str, 
                                 models: Optional[List[str]] = None) -> CounterfactualComparison:
        """Analyze what would happen with different models"""
        models = models or ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo', 'claude-3-sonnet']
        
        scenarios = []
        for model in models:
            scenario = CounterfactualScenario(
                name=f"Use {model}",
                description=f"What if we used {model} instead?",
                type=CounterfactualType.MODEL_CHANGE,
                modifications={'model_name': model},
                expected_outcome=f"Response generated by {model}"
            )
            scenarios.append(scenario)
        
        return self._run_scenarios(execution_id, scenarios)
    
    def analyze_temperature_sensitivity(self, execution_id: str,
                                      temperatures: Optional[List[float]] = None) -> CounterfactualComparison:
        """Analyze sensitivity to temperature changes"""
        temperatures = temperatures or [0.0, 0.3, 0.7, 1.0, 1.5]
        
        scenarios = []
        for temp in temperatures:
            scenario = CounterfactualScenario(
                name=f"Temperature {temp}",
                description=f"What if we used temperature={temp}?",
                type=CounterfactualType.TEMPERATURE_CHANGE,
                modifications={'temperature': temp},
                expected_outcome="More deterministic" if temp < 0.5 else "More creative"
            )
            scenarios.append(scenario)
        
        return self._run_scenarios(execution_id, scenarios)
    
    def analyze_prompt_variations(self, execution_id: str,
                                prompt_variations: List[Dict[str, str]]) -> CounterfactualComparison:
        """Analyze different prompt variations"""
        scenarios = []
        for i, variation in enumerate(prompt_variations):
            scenario = CounterfactualScenario(
                name=f"Prompt Variation {i+1}",
                description=variation.get('description', f"Prompt variation {i+1}"),
                type=CounterfactualType.PROMPT_CHANGE,
                modifications={'replace_inputs': {'0': variation['content']}},
                expected_outcome=variation.get('expected_outcome')
            )
            scenarios.append(scenario)
        
        return self._run_scenarios(execution_id, scenarios)
    
    def analyze_parameter_sweep(self, execution_id: str,
                               parameter: str,
                               values: List[Any]) -> CounterfactualComparison:
        """Analyze a sweep of parameter values"""
        scenarios = []
        for value in values:
            scenario = CounterfactualScenario(
                name=f"{parameter}={value}",
                description=f"What if {parameter} was {value}?",
                type=CounterfactualType.PARAMETER_SWEEP,
                modifications={parameter: value}
            )
            scenarios.append(scenario)
        
        return self._run_scenarios(execution_id, scenarios)
    
    # Note: Cost and quality optimization methods removed in Phase 2.5
    # Focus only on core "what if" scenario analysis
    
    def _enhance_prompt_for_quality(self, original_prompt: str) -> str:
        """Enhance a prompt for better quality"""
        enhancements = [
            "Please provide a detailed and accurate response.",
            "Think step by step and explain your reasoning.",
            "Include specific examples where relevant.",
            "Ensure your response is comprehensive and well-structured."
        ]
        
        return f"{original_prompt}\n\n{' '.join(enhancements)}"
    
    def _run_scenarios(self, execution_id: str, 
                      scenarios: List[CounterfactualScenario]) -> CounterfactualComparison:
        """Run multiple counterfactual scenarios and compare results"""
        results = []
        
        for scenario in scenarios:
            config = ReplayConfiguration(
                modify_llm_params=self._extract_llm_params(scenario.modifications),
                replace_inputs=scenario.modifications.get('replace_inputs'),
                modify_state=self._extract_state_modifications(scenario.modifications)
            )
            
            replay_result = self.replay_engine.replay_execution(execution_id, config)
            
            # Analyze the result
            analysis = self._analyze_scenario_result(scenario, replay_result)
            insights = self._generate_insights(scenario, replay_result, analysis)
            confidence = self._calculate_confidence(scenario, replay_result)
            
            result = CounterfactualResult(
                scenario=scenario,
                replay_result=replay_result,
                analysis=analysis,
                insights=insights,
                confidence=confidence
            )
            results.append(result)
        
        # Find best and worst scenarios
        best_scenario = self._find_best_scenario(results)
        worst_scenario = self._find_worst_scenario(results)
        
        # Generate overall insights and recommendations
        overall_insights = self._generate_overall_insights(results)
        recommendations = self._generate_recommendations(results)
        
        return CounterfactualComparison(
            original_execution_id=execution_id,
            scenarios=results,
            best_scenario=best_scenario,
            worst_scenario=worst_scenario,
            insights=overall_insights,
            recommendations=recommendations
        )
    
    def _extract_llm_params(self, modifications: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Extract LLM parameters from modifications"""
        llm_params = {}
        llm_param_keys = ['model_name', 'temperature', 'max_tokens', 'top_p', 'frequency_penalty', 'presence_penalty']
        
        for key in llm_param_keys:
            if key in modifications:
                llm_params[key] = modifications[key]
        
        return llm_params if llm_params else None
    
    def _extract_state_modifications(self, modifications: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Extract state modifications from modifications"""
        state_mods = {}
        llm_param_keys = ['model_name', 'temperature', 'max_tokens', 'top_p', 'frequency_penalty', 'presence_penalty']
        
        for key, value in modifications.items():
            if key not in llm_param_keys and key != 'replace_inputs':
                state_mods[key] = value
        
        return state_mods if state_mods else None
    
    def _analyze_scenario_result(self, scenario: CounterfactualScenario, 
                               replay_result: ReplayResult) -> Dict[str, Any]:
        """Analyze the result of a counterfactual scenario"""
        analysis = {
            'success': replay_result.success,
            'output_changed': replay_result.output_difference_score > 0.1,
            'significant_change': replay_result.output_difference_score > 0.5,
            'cost_impact': replay_result.cost_difference,
            'performance_impact': replay_result.duration_ms
        }
        
        # Scenario-specific analysis
        if scenario.type == CounterfactualType.MODEL_CHANGE:
            analysis['model_effectiveness'] = self._evaluate_model_change(replay_result)
        elif scenario.type == CounterfactualType.TEMPERATURE_CHANGE:
            analysis['temperature_impact'] = self._evaluate_temperature_change(replay_result)
        elif scenario.type == CounterfactualType.PROMPT_CHANGE:
            analysis['prompt_effectiveness'] = self._evaluate_prompt_change(replay_result)
        
        return analysis
    
    def _evaluate_model_change(self, replay_result: ReplayResult) -> str:
        """Evaluate the effectiveness of a model change"""
        if replay_result.cost_difference > 0:
            return "Higher cost model - evaluate if quality improvement justifies cost"
        elif replay_result.cost_difference < 0:
            return "Lower cost model - monitor quality impact"
        else:
            return "Similar cost - compare quality metrics"
    
    def _evaluate_temperature_change(self, replay_result: ReplayResult) -> str:
        """Evaluate the impact of temperature change"""
        if replay_result.output_difference_score > 0.7:
            return "Significant output variation - high temperature impact"
        elif replay_result.output_difference_score > 0.3:
            return "Moderate output variation - noticeable temperature impact"
        else:
            return "Minimal output variation - low temperature sensitivity"
    
    def _evaluate_prompt_change(self, replay_result: ReplayResult) -> str:
        """Evaluate the effectiveness of prompt changes"""
        if replay_result.output_difference_score > 0.5:
            return "Prompt change significantly affected output"
        else:
            return "Prompt change had minimal impact"
    
    def _generate_insights(self, scenario: CounterfactualScenario,
                          replay_result: ReplayResult, analysis: Dict[str, Any]) -> List[str]:
        """Generate insights from scenario analysis"""
        insights = []
        
        if not replay_result.success:
            insights.append(f"Scenario '{scenario.name}' failed: {replay_result.error}")
            return insights
        
        if analysis['significant_change']:
            insights.append(f"Scenario '{scenario.name}' produced significantly different output")
        
        if replay_result.cost_difference < -0.01:
            savings = abs(replay_result.cost_difference)
            insights.append(f"Could save ${savings:.3f} per execution with this change")
        elif replay_result.cost_difference > 0.01:
            cost = replay_result.cost_difference
            insights.append(f"Would cost additional ${cost:.3f} per execution")
        
        if scenario.type == CounterfactualType.TEMPERATURE_CHANGE:
            temp = scenario.modifications.get('temperature', 'unknown')
            if analysis['temperature_impact'] == "Significant output variation - high temperature impact":
                insights.append(f"Temperature {temp} creates high variability - use for creative tasks")
            elif temp <= 0.3:
                insights.append(f"Temperature {temp} produces consistent outputs - good for factual tasks")
        
        return insights
    
    def _calculate_confidence(self, scenario: CounterfactualScenario,
                            replay_result: ReplayResult) -> float:
        """Calculate confidence in the counterfactual result"""
        confidence = 0.5  # Base confidence
        
        if replay_result.success:
            confidence += 0.3
        
        if replay_result.output_difference_score > 0:
            confidence += 0.1  # Output actually changed
        
        # Reduce confidence for complex scenarios
        if len(scenario.modifications) > 2:
            confidence -= 0.1
        
        return max(0.0, min(1.0, confidence))
    
    def _find_best_scenario(self, results: List[CounterfactualResult]) -> Optional[CounterfactualResult]:
        """Find the best performing scenario - simplified scoring"""
        successful_results = [r for r in results if r.replay_result.success]
        if not successful_results:
            return None
        
        # Simple scoring based on confidence and output difference
        def score_scenario(result: CounterfactualResult) -> float:
            score = result.confidence * 10
            # Prefer scenarios with meaningful output changes
            if result.replay_result.output_difference_score > 0.1:
                score += 5
            return score
        
        return max(successful_results, key=score_scenario)
    
    def _find_worst_scenario(self, results: List[CounterfactualResult]) -> Optional[CounterfactualResult]:
        """Find the worst performing scenario"""
        if not results:
            return None
        
        # Prioritize failed scenarios, then low confidence
        failed_results = [r for r in results if not r.replay_result.success]
        if failed_results:
            return failed_results[0]
        
        def score_scenario(result: CounterfactualResult) -> float:
            return -result.confidence  # Lower confidence = worse
        
        return max(results, key=score_scenario)
    
    def _generate_overall_insights(self, results: List[CounterfactualResult]) -> List[str]:
        """Generate insights across all scenarios - simplified for Phase 2.5"""
        insights = []
        
        successful_count = sum(1 for r in results if r.replay_result.success)
        insights.append(f"{successful_count}/{len(results)} scenarios executed successfully")
        
        high_variance_scenarios = [r for r in results 
                                 if r.replay_result.success and r.replay_result.output_difference_score > 0.7]
        if high_variance_scenarios:
            insights.append(f"{len(high_variance_scenarios)} scenarios showed significant output changes")
        
        return insights
    
    def _generate_recommendations(self, results: List[CounterfactualResult]) -> List[str]:
        """Generate actionable recommendations - simplified for Phase 2.5"""
        recommendations = []
        
        best = self._find_best_scenario(results)
        if best:
            recommendations.append(f"Consider implementing '{best.scenario.name}' based on analysis")
        
        failed_scenarios = [r for r in results if not r.replay_result.success]
        if failed_scenarios:
            recommendations.append("Some scenarios failed - review error handling and input validation")
        
        # Focus on output variation insights
        high_change_scenarios = [r for r in results 
                               if r.replay_result.success and r.replay_result.output_difference_score > 0.5]
        if len(high_change_scenarios) > len(results) * 0.5:
            recommendations.append("Multiple scenarios produced significant output changes - review sensitivity")
        
        return recommendations
